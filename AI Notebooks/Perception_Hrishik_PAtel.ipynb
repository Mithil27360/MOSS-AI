{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e13f2c19-321a-4bf0-8813-ab942edb59db",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "## The structural building block of deep learning\n",
    "The idea of a perceptron (or a single neuron) is a fundamental building block of a neural network.\n",
    "\n",
    "It can be defined by its forward propogation of information, it is the product of a Non-Linear activation function with the linear combination of inputs added with a bias term. The constants used during the linear Combination are called \"Weights\". The bias term allows us to shift left and right along the activation function. So this is just a shifting scaler designed within the equation. Bias helps us to handle the data sets where the classes are not centred about the origin. For simplicity I will assume the output is z (that is z is the input for the activation function)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*5a_Ubdxg86cybTVhJAfWow.png\" alt=\"Perceptron with Input, Weights, Summation, and Activation Function\">\n",
    "\n",
    "## Activation Functions\n",
    "The point of Activation functions is to introduce non linearities because real data in the real world is heavily non linear.\n",
    "\n",
    "Non linearities (activation functions which are non linear) helps us to approximate arbitarily complex functions with enough depth in a model\n",
    "\n",
    "Common actication functions are the sigmoid, hyperbolic tangent. \n",
    "\n",
    "in TensorFlow it is tf.math.sigmoid(z) &  tf.math.tanh(z)\n",
    "\n",
    "in Torch it is torch.sigmoid(z) & torch.tanh(z)\n",
    "\n",
    "## Building Neural Networks with Perceptrons\n",
    "First we will build multi output perceptrons. Both neurons have the same input but because the weights are different the outputs are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bbba44-a75a-4182-8d92-20e8bb185116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight([input_dim, output_dim])\n",
    "        self.b = self.add_weight([1, output_dim])\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.w) + self.b\n",
    "        output = tf.math.sigmoid(z)\n",
    "        return output\n",
    "        #this is the manual way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93df8e98-08f0-49b0-b3ad-37b6367a2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(units=2)\n",
    "#this is a function which you can call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3ac99c-96a9-4778-be08-4355e63dd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi output perception\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951baa3-59f5-412b-b4ae-716057d2ee85",
   "metadata": {},
   "source": [
    "More complex tasks require more depth so you introduce more hierachal non linearities, after one layer if you have a single dense connection followed by a non linearity you have a limited amount of complexity that you can extract cause it is only coming from one none linearity so it is limited to the expressive capacity of a single non linearity. as you get more and more complex tasks you require deeper and deeper expressive functions. More outputs implies your need to predict more things.\n",
    "\n",
    "For an example to generate an image you require to generate values for every pixel of theat image that implies a lot of outputs.\n",
    "\n",
    "## Applying Neural Networks\n",
    "Without Training a neural network it is like a baby that has no knowledge about the current world as it doesn't know anything about the problem and it needs to first learn about the problem.\n",
    "\n",
    "In order to train our model it first needs to understand when it makes bad predictions. A bad prediction means that it has to be able to quantify how bad the prediction is and how good a prediction is. This is called a loss in a neural network.\n",
    "The loss will be a measure of how far its predictions are from the ground truth. Smaller the loss closer the truth and prediction is. Empirical loss measures the total loss over our entire dataset. Cross Entropy Loss can be used with models that output a probability between 0 and 1. Mean Squared Error Loss can be used with regression models that ouput continuos real numbers.\n",
    "\n",
    "## Training Neural Networks\n",
    "We want to find the network weights that achieve the minimal loss in our entire dataset.\n",
    "\n",
    "Loss Optimization is a function of the network weights. To do so we use a method called Gradient Descent\n",
    "<img src=\"https://global.discourse-cdn.com/dlai/optimized/3X/f/5/f58df86a4c92695569d9536d7e752161cd0f98fb_2_690x371.jpeg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a6aed-f1c9-49d1-99b2-ae0e04d43d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable([tf.random.normal()])\n",
    "while True:\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = compute_loss(weights)\n",
    "        gradient = g.gradient(loss, weights)\n",
    "    weights=weights - lr*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a5b94-040a-47ff-8bb0-b5fde8cea7b3",
   "metadata": {},
   "source": [
    "## Neural Networks in Practice\n",
    "Loss Functions Can Be Difficult to Optimize.\n",
    "\n",
    "Learning rate decides how quick does a gradient move in backword propogation.\n",
    "If we set the learning rate too slow then we basically start from the point and we gt stuck in some of the local minimum that may not be the best minimum that we can get up to.\n",
    "If we set it too Large then we overshoot that is we start to step in the right direction and then we explode out of the stable paces of learning.\n",
    "\n",
    "### How do we set the learning rate?\n",
    "Build a design that adapts itself while optimising. That is your learning rates will increase or decrease as a function of the gradients and the data.\n",
    "Many adaptive learning rates have been build and stroed in TF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1cc0f4-de55-41b0-90f5-ecc95a9b12ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam\n\u001b[1;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdadelta\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.optimizers.SGD\n",
    "tf.keras.optimizers.Adam\n",
    "tf.keras.optimizers.Adadelta\n",
    "tf.keras.optimizers.Adagrad\n",
    "tf.keras.optimizers.RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5e61a-ff0a-4437-8fd9-db840cf635d3",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "Ideally in ML we don't want to train models that work good only in our training set. What we want is that it should work well in a brand new dataset. We use Training data as a proxy to train it to work well on a new dataset that is an unseen test data.\n",
    "<img src=\"https://miro.medium.com/v2/1*_7OPgojau8hkiPUiHoGK_w.png\">\n",
    "You want is to end up in the middle that is to record your training points but not rely on them too much or memorize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ce293-3a96-4464-a520-1853d2946ffd",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Durin training we randomly set activations to 0 like dropout to 25%, so what it will essentially do is say 25% of our neurons will dropout from the activation function that forces the network to not rely so much on the outputs of any one neuron. Even if we put the same data twice and put it to the model twice it will not be able to remember it due to the dropout and in return will increase its stoicasticity.\n",
    "Early stopping is to stop before over training a dataset to reduce chances of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0399943-a211-451e-b38f-e5715c761028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
